# Use unique initializations for better diversity
seed: null
ignore_warnings: True

multisession: True

model:
  _target_: lfads_torch.models.base_model.LFADS
  # --------- architecture --------- #
  encod_data_dim: ${model.fac_dim}
  encod_seq_len: 140
  recon_seq_len: 180
  ext_input_dim: 0
  ic_enc_seq_len: 0
  ic_enc_dim: 100
  ci_enc_dim: 80
  ci_lag: 1
  con_dim: 80
  co_dim: 4
  ic_dim: 100
  gen_dim: 100
  fac_dim: 40
  # --------- readin / readout --------- #
  readin:
    _target_: torch.nn.ModuleList
    modules:
      - _target_: torch.nn.Linear
        in_features: 122
        out_features: ${model.fac_dim}
      - _target_: torch.nn.Linear
        in_features: 114
        out_features: ${model.fac_dim}
      - _target_: torch.nn.Linear
        in_features: 107
        out_features: ${model.fac_dim}

  readout:
    _target_: torch.nn.ModuleList
    modules:
      - _target_: torch.nn.Linear
        in_features: ${model.fac_dim}
        out_features: 162
      - _target_: torch.nn.Linear
        in_features: ${model.fac_dim}
        out_features: 152
      - _target_: torch.nn.Linear
        in_features: ${model.fac_dim}
        out_features: 142
  # --------- augmentation --------- #
  train_aug_stack:
    _target_: lfads_torch.models.modules.augmentations.AugmentationStack
    transforms:
      - _target_: lfads_torch.models.modules.augmentations.CoordinatedDropout
        cd_rate: 0.3
        cd_pass_rate: 0.0
        ic_enc_seq_len: ${model.ic_enc_seq_len}
    batch_order: [0]
    loss_order: [0]
  infer_aug_stack:
    _target_: lfads_torch.models.modules.augmentations.AugmentationStack
    transforms:
      - _target_: lfads_torch.models.modules.augmentations.IgnoreNaNLoss
        encod_data_dim: ${model.encod_data_dim}
        encod_seq_len: ${model.encod_seq_len}
        scale_by_quadrant: False
    loss_order: [0]
  # --------- priors / posteriors --------- #
  reconstruction:
    - _target_: lfads_torch.models.modules.recons.Poisson
    - _target_: lfads_torch.models.modules.recons.Poisson
    - _target_: lfads_torch.models.modules.recons.Poisson
  variational: True
  co_prior:
    _target_: lfads_torch.models.modules.priors.AutoregressiveMultivariateNormal
    tau: 10.0
    nvar: 0.1
    shape: ${model.co_dim}
  ic_prior:
    _target_: lfads_torch.models.modules.priors.MultivariateNormal
    mean: 0
    variance: 0.1
    shape: ${model.ic_dim}
  ic_post_var_min: 1.0e-4
  # --------- misc --------- #
  dropout_rate: 0.02 # sampled
  cell_clip: 5.0
  loss_scale: 1.0e+4
  recon_reduce_mean: True

  # --------- learning rate --------- #
  lr_init: 4.0e-3
  lr_stop: 1.0e-5
  lr_decay: 0.95
  lr_patience: 6
  lr_adam_beta1: 0.9
  lr_adam_beta2: 0.999
  lr_adam_epsilon: 1.0e-8
  lr_scheduler: False

  # --------- regularization --------- #
  weight_decay: 0.0
  l2_start_epoch: 0
  l2_increase_epoch: 80
  l2_ic_enc_scale: 0.0
  l2_ci_enc_scale: 0.0
  l2_gen_scale: 0.0 # sampled
  l2_con_scale: 0.0 # sampled
  kl_start_epoch: 0
  kl_increase_epoch: 80
  kl_ic_scale: 0.0 # sampled
  kl_co_scale: 0.0 # sampled

datamodule:
  _target_: lfads_torch.datamodules.BasicDataModule
  data_paths:
    - ${relpath:datasets/mc_maze_large-05ms-val.h5}
    - ${relpath:datasets/mc_maze_medium-05ms-val.h5}
    - ${relpath:datasets/mc_maze_small-05ms-val.h5}
  batch_keys:
    - behavior
  attr_keys:
    - psth
    - train_cond_idx
    - valid_cond_idx
    - train_decode_mask
    - valid_decode_mask
  batch_size: 192

pbt:
  metric: valid/recon_smth
  resources_per_trial:
    cpu: 2
    gpu: 0.5
  num_samples: 20
  perturbation_interval: 25
  burn_in_period: 105 # ramping + perturbation_interval
  quantile_fraction: 0.25
  resample_probability: 0.25
  hps:
    model.lr_init:
      init_range: [4.0e-3, 4.0e-3]
      resamp_range: [1.0e-4, 5.0e-3]
      dist: loguniform
    model.train_aug_stack.transforms.0.cd_rate:
      init_range: [0.5, 0.5]
      resamp_range: [0.01, 0.7]
      dist: uniform
    model.dropout_rate:
      init_range: [0.1, 0.1]
      resamp_range: [0.0, 0.2]
      dist: uniform
    model.kl_co_scale:
      init_range: [1.0e-6, 1.0e-3]
      resamp_range: [1.0e-6, 1.0e-3]
      dist: loguniform
    model.kl_ic_scale:
      init_range: [1.0e-6, 1.0e-3]
      resamp_range: [1.0e-6, 1.0e-3]
      dist: loguniform
    model.weight_decay:
      init_range: [1.0e-10, 1.0e-2]
      resamp_range: [1.0e-10, 1.0e-2]
      dist: loguniform

trainer:
  _target_: pytorch_lightning.Trainer
  gradient_clip_val: 200
  max_epochs: 1_000
  log_every_n_steps: 5
  # Checkpointing is managed by the `TuneReportCheckpointCallback`
  enable_checkpointing: False
  # Prevent console output by individual models
  enable_progress_bar: False
  enable_model_summary: False

callbacks:
  tune_report_checkpoint_callback:
    _target_: ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback
    metrics:
      - valid/recon_smth
      - cur_epoch
    filename: tune.ckpt
    "on": validation_end
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch
  raster_plot:
    _target_: lfads_torch.callbacks.RasterPlot
    log_every_n_epochs: 200
  trajectory_plot:
    _target_: lfads_torch.callbacks.TrajectoryPlot
    log_every_n_epochs: 200

logger:
  csv_logger:
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: "csv_logs"
    # Allow the logger to create a new version for each generation
    version: null
    name: ""
  tensorboard_logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    # Configure logger so PTL and ray use the same TB file.
    save_dir:
      _target_: ray.tune.get_trial_dir
    version: ""
    name: ""

posterior_sampling:
  use_best_ckpt: False
  fn:
    _target_: lfads_torch.post_run.analysis.run_posterior_sampling
    filename: lfads_output.h5
    num_samples: 50
